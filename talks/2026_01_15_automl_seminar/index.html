<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Stop Guessing, Start Discovering Trade-offs in Your ML Models</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<style>
			.full {
				height: 100%;
				width: 100%;
			}
			.credit {
				bottom: 10px;
				left: 0px;
				position: absolute;
				font-size: large;
				text-align: left;
			}
			.highlight-red {
				color: #ff2c2d;
			}
			.highlight-green {
				color: #3CB043;
			}
			.container{
				display: flex;
			}
			.col{
				flex: 1;
			}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h3>Stop Guessing: Discover Optimal Trade-offs in Your ML Models</h3>
					<p>
						<small>Andrei Paleyes</small>
					</p>
					<p>AutoML Seminar</p>
					<p>January 2026</p>
					<aside class="notes">
						Hi! Thanks for the intro, it's great being here and presenting at this amazing seminar series.
					</aside>
				</section>
				<section>
					<table>
						<tr>
							<td>
								<img src="images/brendan.jpg" style="width: 200px; display: block;"/>
								Brendan Avent
							</td>
							<td>
								<img src="images/bogdan.jpg" style="width: 200px; display: block;"/>
								Bogdan Ficiu
							</td>
							<td>
								<img src="images/emile.png" style="width: 200px; display: block;"/>
								Emile Ferreira
							</td>
						</tr>
					</table>
					<aside class="notes">
						First off, credit where it's due. Everything we'll cover today is primarily the result of a work of these amazing people, whom I was fortunate enough to supervise at some point as students or interns.
					</aside>
				</section>
				<section>
					<p>Discovering and navigating trade-offs in ML models...</p>
					
					<p class="fragment">Sounds familiar?</p>
					<aside class="notes">
						Today we are going to talk about solid way of discovering and navigating trade-offs in ML models. If you are a regular of this seminar this may sound familiar to you.
					</aside>
				</section>
				<section>
					<div style="height:99vh">
						<img src="images/jan-talk.png" style="height: 70vh" />
						<p class="credit">
							"Multi-Objective AutoML: Towards Accurate and Robust models", Jan van Rijn, AutoML seminar, 16 Oct 2025
						</p>
					</div>
					<aside class="notes">
						Indeed, just a few months ago Jan van Rijn gave a talk here about a trade-off between accuracy and robustness. Great talk, please check it out if you missed it, i think it greatly complements today's presentation.
					</aside>
				</section>
				<section>
					<h3>Today's highlights</h3>
					<p class="fragment">3 objectives: fairness, privacy, energy efficiency</p>
					<p class="fragment">Multi-objective Bayesian Optimisation (MOBO)</p>
					<p class="fragment">Step-by-step guide!</p>
					<aside class="notes">
						But of course today's talk will be different. Here are the highlights. First, we'll talk not about only one objective, but three: fairness, privacy, energy efficiency. Second, we'll be using multi-objective Bayesian optimisation as the discovery mechanism. And third, in the end I will give you a step by step recipe that you can use to discover the trade-offs that matter the most in your context. This might include privacy, robustness, speed, or anything else you fancy.
					</aside>
				</section>
				<section>
					<h3>Two models dilemma</h3>
					<div class="container">
						<div class="col">
							<b>Model A</b>
							<ul>
								<li>96% accuracy</li>
								<li>15 samples per juole</li>
							</ul>
						</div>
						<div class="col">
							<b>Model B</b>
							<ul>
								<li>89% accuracy</li>
								<li>21 samples per juole</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						Let's consider a situation. We have two models, one is a bit more accurate, another consumes less energy. Which do you deploy? Which one is better?

						The right answer is "it depends". Depending on the circumstances, such as regulations, or customer requirements, or budgets, answer might be different. Which means it is crucial to have this sort of information readily at hand. Yet more often than not it is not available at all.
					</aside>
				</section>
				<section>
					grid or random search image
					<aside class="notes">
						What do people normally do when they want to discover these options? They do a grid search, or random search, or a bayesian optimisation run, optimising for one objective, say accuracy, and then afterwards they measure other properties of the models that were evaluated during the search. This is better than nothing, but it has several drawbacks. First, since the search was optimising for one objective only, the discovered models are likely to be suboptimal in terms of other objectives. Second, since the search was not aware of multiple objectives, it is likely to waste a lot of resources exploring areas of the search space that are not interesting.
					</aside>
				</section>
				<section>
					<div style="height: 99vh">
						<div class="container">
							<div class="col"><img src="images/privacy_constraints.png" style="height: 60vh;" /></div>
							<div class="col"><img src="images/fairness_constraints.png" style="height: 90vh" /></div>
						</div>
						<div class="credit">Table sources: Xu et al. 2019 "Achieving Differential Privacy and Fairness in Logistic Regression", Chang and Shokri 2020 "On the Privacy Risks of Algorithmic Fairness"</div>
					</div>
					<aside class="notes">
						Another approach is to do a constraint training. For example, you can try to maximise accuracy while constraining energy consumption to be below a certain threshold. For exampl here we can see two examples from academic papers. On the left researchers have set the constraint on differential privacy measure, and on the right the constraint is on fairness. Both papers then proceed to train models under these constraints, and report training results, which I've clipped here for clarity. This is better than the previous approach, since the search is now aware of multiple objectives. However, it has its own drawbacks. First, you need to know the constraints beforehand, which is not always possible. Second, you don't know if the constraints you set are too tight or too loose, which may lead to suboptimal results.
					</aside>
				</section>
				<section>
					entire trade off surface. how?
					<aside class="notes">
						Of course the best approach is to understand the entire trade-off surface, and then pick the model that best suits your needs. But how does it look like, and how to discover it efficiently?
					</aside>
				</section>
				<section>
					<div style="height:99vh">
						<img src="images/us-census-pareto-front.png" style="height: 70vh" />
						<p class="credit">
							"Disclosure avoidance for block level data and protection of confidentiality in public tabulations.", John M. Abowd, Census Scientific Advisory Committee (Fall Meeting), 2018
						</p>
					</div>
					<aside class="notes">
						As for "how does it look like", in most cases it looks like this: a set of optimal trade-offs, known as Pareto front, where improving one objective necessarily leads to degradation in another objective. Our goal is to discover this front as efficiently as possible. However you can see a slide from a talk from John M. Abowd, Chief Scientist at the US Census Bureau, where he discusses trade-offs between privacy and accuracy in the context of census data.
					</aside>
				</section>
				<section>
					MOBO
					<aside class="notes">
						So how do we discover this Pareto front efficiently? The answer I propose is multi-objective Bayesian optimisation, or MOBO for short.
					</aside>
				</section>
				<section>
					1 dim BO recap
					<aside class="notes">
						I am going to assume most people watching live or on the channel are familiar with single-objective BayesOpt, so just a one-slide refresher here. Bayesian Optimisation is an optimisation method for expensive black-box functions. We model the function with a probabilistic model, typically a Gaussian process, and use this model to construct an acquisition function. Acquisition function is used to guide out evaluations of the objective. The hope is that by balancing exploration of the input space with exploitation of the most promising regions we will eventually arrive at a global extremum.
					</aside>
				</section>
				<section>
					ref: https://www.sciencedirect.com/science/article/pii/S0167947315001991#s000015
					<aside class="notes">
						Now let's move on to the multi-objective case, which presumably less people are familiar with. So what are the major differences? Again we have an objective function, but now it is vector output, not a scalar. For simplicity let's assume the vector is of length 2, meaning we have 2 competing measures to optimise. Again we want to model our objective with a GP. Two approaches are possible: either a single multi-output GP, or two separate single output GPs. We chose the latter as it was shown to be fairly competitive while much simpler to compute. And again we use an acquisition function to guide our evaluations. But the acquisition function needs to be a scalar. And this leads us to what i believe is the key idea of MOBO - hypervolume.
					</aside>
				</section>
				<section>
					pareto front illustration
					<aside class="notes">
						Here is a simple example. Suppose we have these few evaluations of our objective, depicted in the objective space. We can build a Pareto front around these points - that's our current best. What we would like to do is to extend this frontier as much as possible. This point does not extend the frontier, as it dominated by other points. But this one does, and so does that one. Which one if better? Well, if we consider our objectives to have equal scales, we can posit that the better point is the one that increases the area covered by the Pareto front more.
					</aside>
				</section>
				<section>

					<aside class="notes">
						How to calculate this area? We need to pick a reference, or an anti-ideal, point first - a point somewhere in space that is guaranteed to be pretty bad in both objectives. This allows us to calculate the area and pick the point that covers more. THat's the intuition for a hypervolume.
					</aside>
				</section>
				<section>

					<aside class="notes">
						In our projects we used two acqusition functions that build on the ideal of hypervolume. Hypervolume Probability of Improvement (HVPoI) clearly aims to maximise the probability the chosen point increases the HV. Expected Hypervolume Improvement (EHVI) maximises the expectated improvement. Both are directly inspired by their single-objective counterparts.
					</aside>
				</section>
				<section>

					<aside class="notes">
						Few keys points to remember about MOBO.
						1. Keep scales comparable, normalise objective's values if necessary
						2. Reference point matters
						3. EHVI is a good practical choice
					</aside>
				</section>
				<section>

					<aside class="notes">
						Hopefully this gives everyone some MOBO intuition. Now let's move on to the exciting part - case studies!
					</aside>
				</section>
				<section>
					Case study 1: DPareto - Utility vs privacy 
					<aside class="notes">
						The context of this first case study is training ML models on sensitive data. So we really want to protect our models against memberhsip attacks. A standard mechanism for this is differential privacy. Intuitively we distort our training data which makes it harder to identify individuals from inference outputs. But of course the more distrotion we introduce the less accurately we are modelling the training data. Hence the trade-off!
					</aside>
				</section>
				<section>

					<aside class="notes">
						To quantify this trade-off we picked a metric to measure model performance, and used epsilon as a common way to measuing the level of privacy achieved. We also defined a space of hyperparameters, including some common ones (batch size, number of epochs) and some DP-specific (clipping norm, noise level).
					</aside>
				</section>
				<section>

					<aside class="notes">
						This was the first time we were trying this mechanism, so the main question was "Will it all work?". It did, and ...
					</aside>
				</section>
				<section>
					Case study 2: PFairDP - utility vs privacy vs fairness
					<aside class="notes">
						3d fronts, exciting! This second study is a natural extension of the first, were we added one more objective to the optimisation problem: algorithmic fairness. You might be aware of the fact that there are over 20 definitions of fairness, and we managed to build a pipeline that would work with any of these that can be tuned with a hyperparameter.
					</aside>
				</section>
				<section>

					<aside class="notes">
						
					</aside>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
