<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Stop Guessing, Start Discovering Trade-offs in Your ML Models</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<style>
			.full {
				height: 100%;
				width: 100%;
			}
			.credit {
				bottom: 10px;
				left: 0px;
				position: absolute;
				font-size: large;
				text-align: left;
			}
			.highlight-red {
				color: #ff2c2d;
			}
			.highlight-green {
				color: #3CB043;
			}
			.container{
				display: flex;
			}
			.col{
				flex: 1;
			}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h3>Stop Guessing: Discover Optimal Trade-offs in Your ML Models</h3>
					<p>
						<small>Andrei Paleyes</small>
					</p>
					<p>AutoML Seminar</p>
					<p>January 2026</p>
					<aside class="notes">
						Hi! Thanks for the intro, it's great being here and presenting at this amazing seminar series.
					</aside>
				</section>
				<section>
					<table>
						<tr>
							<td>
								<img src="images/brendan.jpg" style="width: 200px; display: block;"/>
								Brendan Avent
							</td>
							<td>
								<img src="images/bogdan.jpg" style="width: 200px; display: block;"/>
								Bogdan Ficiu
							</td>
							<td>
								<img src="images/emile.png" style="width: 200px; display: block;"/>
								Emile Ferreira
							</td>
						</tr>
					</table>
					<aside class="notes">
						First off, credit where it's due. Everything we'll cover today is primarily the result of a work of these amazing people, whom I was fortunate enough to supervise at some point as students or interns.
					</aside>
				</section>
				<section>
					<p>Discovering and navigating trade-offs in ML models...</p>
					
					<p class="fragment">Sounds familiar?</p>
					<aside class="notes">
						Today we are going to talk about solid way of discovering and navigating trade-offs in ML models. If you are a regular of this seminar this may sound familiar to you.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<img src="images/jan-talk.png" style="height: 70vh" />
						<p class="credit">
							"Multi-Objective AutoML: Towards Accurate and Robust models", Jan van Rijn, AutoML seminar, 16 Oct 2025
						</p>
					</div>
					<aside class="notes">
						Indeed, just a few months ago Jan van Rijn gave a talk here about a trade-off between accuracy and robustness. Great talk, please check it out if you missed it, i think it greatly complements today's presentation.
					</aside>
				</section>
				<section>
					<h4>Today's highlights</h4>
					<p class="fragment">3 objectives: fairness, privacy, energy efficiency</p>
					<p class="fragment">Multi-objective Bayesian Optimisation (MOBO)</p>
					<p class="fragment">Step-by-step guide!</p>
					<aside class="notes">
						But of course today's talk will be different. Here are the highlights. First, we'll talk not about only one objective, but three: fairness, privacy, energy efficiency. Second, we'll be using multi-objective Bayesian optimisation as the discovery mechanism. And third, in the end I will give you a step by step recipe that you can use to discover the trade-offs that matter the most in your context. This might include privacy, robustness, speed, or anything else you fancy.
					</aside>
				</section>
				<section>
					<h4>Two models dilemma</h4>
					<div class="container">
						<div class="col">
							<b>Model A</b>
							<ul>
								<li>96% accuracy</li>
								<li>15 samples per juole</li>
							</ul>
						</div>
						<div class="col">
							<b>Model B</b>
							<ul>
								<li>89% accuracy</li>
								<li>21 samples per juole</li>
							</ul>
						</div>
					</div>
					<aside class="notes">
						Let's consider a situation. We have two models, one is a bit more accurate, another consumes less energy. Which do you deploy? Which one is better?

						The right answer is "it depends". Depending on the circumstances, such as regulations, or customer requirements, or budgets, answer might be different. Which means it is crucial to have this sort of information readily at hand. Yet more often than not it is not available at all.
					</aside>
				</section>
				<section>
					<div style="height:99vh">
						<h4>Option 1: grid or random search</h4>
						<img src="images/grid-vs-random.png" />
					</div>
					<aside class="notes">
						What do people normally do when they want to discover these options? They do a grid search or random search in the hyperparameter space, train the model for each poitn, and pick whatever option seems to satisfy the requirements best. Of course this is a very naive approach, but it is nonetheless possible and can sometimes yield decent results. Its major drawback is that it compeltely ignores the objectives we set out to optimise. Also, it is likely to waste a lot of resources exploring areas of the search space that are not interesting.
					</aside>
				</section>
				<section>
					<div style="height: 90vh">
						<h4>Option 2: constrained optimisation</h4>
						<div class="container">
							<div class="col"><img src="images/privacy_constraints.png" style="height: 60vh;" /></div>
							<div class="col"><img src="images/fairness_constraints.png" style="height: 70vh" /></div>
						</div>
						<div class="credit">Table sources: Xu et al. 2019 "Achieving Differential Privacy and Fairness in Logistic Regression", Chang and Shokri 2020 "On the Privacy Risks of Algorithmic Fairness"</div>
					</div>
					<aside class="notes">
						Another approach is to do a constraint training. We maximise accuracy while constraining other objectives to be below a certain threshold. Here we can see two examples from academic papers. On the left researchers have set the constraint on differential privacy measure, and on the right the constraint is on fairness. Both papers then proceed to train models under these constraints, and report training results, which I've clipped here for clarity. This is better than the previous approach, since the search is now aware of multiple objectives. However, it has its own drawbacks. First, you need to know the constraints beforehand, which is not always possible. Second, you don't know if the constraints you set are too tight or too loose, which may lead to suboptimal results.
					</aside>
				</section>
				<section>
					<h2>Can we discover an entire trade-off surface?</h2>
					<aside class="notes">
						Of course the best approach is to understand the entire trade-off surface, and then pick the model that best suits your needs. But how does it look like, and how to discover it efficiently?
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<h4>Pareto front</h4>
						<img src="images/us-census-pareto-front.png" style="height: 60vh" />
						<p class="credit">
							"Disclosure avoidance for block level data and protection of confidentiality in public tabulations.", John M. Abowd, Census Scientific Advisory Committee (Fall Meeting), 2018
						</p>
					</div>
					<aside class="notes">
						As for "how does it look like", in most cases it looks like this: a set of optimal trade-offs, known as Pareto front, where improving one objective necessarily leads to degradation in another objective. Our goal is to discover this front as efficiently as possible. However you can see a slide from a talk from John M. Abowd, Chief Scientist at the US Census Bureau, where he discusses trade-offs between privacy and accuracy in the context of census data.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<h4>Option 3: Multi-objective Bayesian optimisation (MOBO)</h4>
						<img src="images/mobo_intro.png" style="height: 60vh" />
						<p class="credit">
							Image source: "High-Dimensional Bayesian Multi-Objective Optimization", Gaudrie, 2019
						</p>
					</div>
					<aside class="notes">
						So how do we discover this Pareto front efficiently? The answer I propose is multi-objective Bayesian optimisation, or MOBO for short.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<h4>Bayesian optimisation recap</h4>
						<img src="images/1d-bayes-opt.png" style="height: 60vh" />
						<p class="credit">
							Image source: "Bayesian Optimization and Active Learning Cookbook", Sam Lishak, Physics X, 2025
						</p>
					</div>
					<aside class="notes">
						I am going to assume most people watching live or on the channel are familiar with single-objective BayesOpt, so just a one-slide refresher here. Bayesian Optimisation is an optimisation method for expensive black-box functions. We model the function with a probabilistic model, typically a Gaussian process, and use this model to construct an acquisition function. Acquisition function is used to guide out evaluations of the objective. The hope is that by balancing exploration of the input space with exploitation of the most promising regions we will eventually arrive at a global extremum.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<h4>MOBO: differences</h4>
						Assuming 2 objectives
						<ul>
							<li>f(x) = (y<sub>1</sub>, y<sub>2</sub>)</li>
							<li>Model: multi-output GPs or 2 single-output GPs</li>
							<li>Hypervolume-based acquisition function</li>
						</ul>
						<p class="credit">
							"Multiobjective optimization of expensive-to-evaluate deterministic computer simulator models", Svenson and Santner, Computational Statistics & Data Analysis, 2016
						</p>
					</div>
					<aside class="notes">
						Now let's move on to the multi-objective case, which presumably less people are familiar with. What are the major differences? Again we have an objective function, but now it is vector output, not a scalar. For simplicity let's assume the vector is of length 2, meaning we have 2 competing measures to optimise. Again we want to model our objective with a GP. Two approaches are possible: either a single multi-output GP, or two separate single output GPs. We chose the latter as it was shown to be fairly competitive while much simpler to compute. And again we use an acquisition function to guide our evaluations. But the acquisition function needs to be a scalar. And this leads us to what i believe is the key idea of MOBO - hypervolume.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<img src="images/pareto-1.png" />
					</div>
					<aside class="notes">
						Here is a simple example. Suppose we have these few evaluations of our objective, depicted in the objective space.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<img src="images/pareto-2.png" />
					</div>
					<aside class="notes">
						We can build a Pareto front around these points - that's our current best. What we would like to do is to extend this frontier as much as possible.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<img src="images/pareto-3.png" />
					</div>
					<aside class="notes">
						Point 3 does not extend the frontier, as it dominated by other points. But points 1 and 2 can are not dominated. Which one is better? Well, if we consider our objectives to have equal scales, we can posit that the better point is the one that increases the area covered by the Pareto front more.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<img src="images/pareto-4.png" />
					</div>
					<aside class="notes">
						How to calculate this area? We need to pick a reference, or an anti-ideal, point first - a point somewhere in space that is guaranteed to be pretty bad in both objectives. Here it is denoted with R.From this point we can calculate the area covered by the Pareto front.This allows us to calculate the area and pick the point that covers more. That's the intuition for a hypervolume.
					</aside>
				</section>
				<section>
					<div style="height:90vh">
						<img src="images/pareto-5.png" />
					</div>
					<aside class="notes">
						Whichever point adds larger area to is better for us. That's the intuition for a hypervolume-based optimisation.
					</aside>
				</section>
				<section>
					TODO:
					HVPoI
					\boxed{
\alpha_{\mathrm{HV\text{-}PI}}(\mathbf{x}) = \mathbb{P}\left(\mathrm{HVI}(\mathbf{Y}) > 0\right) = \int_{\mathcal{R}_{\mathrm{HVI}>0}}p(\mathbf{y} \mid \mathbf{x}, \mathcal{D}) , d\mathbf{y}}

EHVI
\boxed{
\alpha_{\mathrm{EHVI}}(\mathbf{x}) = \mathbb{E}\left[ \mathrm{HVI}(\mathbf{Y}) \right] = \int_{\mathbb{R}^M}\mathrm{HVI}(\mathbf{y}) ,p(\mathbf{y} \mid \mathbf{x}, \mathcal{D}), d\mathbf{y}}
					<aside class="notes">
						In our projects we used two acqusition functions that build on the ideal of hypervolume. Hypervolume Probability of Improvement (HVPoI) clearly aims to maximise the probability the chosen point increases the HV. Expected Hypervolume Improvement (EHVI) maximises the expectated improvement. Both are directly inspired by their single-objective counterparts.
					</aside>
				</section>
				<section>

					<aside class="notes">
						Few keys points to remember about MOBO.
						1. Keep scales comparable, normalise objective values if necessary
						2. Reference point matters
						3. EHVI is a good practical choice
					</aside>
				</section>
				<section>

					<aside class="notes">
						Hopefully this gives everyone some MOBO intuition. Now let's move on to the exciting part - case studies!
					</aside>
				</section>
				<section>
					Case study 1: DPareto - Utility vs privacy 
					<aside class="notes">
						The context of this first case study is training ML models on sensitive data. So we really want to protect our models against memberhsip attacks. A standard mechanism for this is differential privacy. Intuitively we distort our training data which makes it harder to identify individuals from inference outputs. But of course the more distrotion we introduce the less accurately we are modelling the training data. Hence the trade-off!
					</aside>
				</section>
				<section>

					<aside class="notes">
						To quantify this trade-off we picked a metric to measure model performance, and used epsilon as a common way to measuing the level of privacy achieved. We also defined a space of hyperparameters, including some common ones (batch size, number of epochs) and some DP-specific (clipping norm, noise variance).
					</aside>
				</section>
				<section>

					<aside class="notes">
						This was the first time we were trying this mechanism, so the main question was "Will it all work?". It did! We were able to construct Pareto fronts using different DP algorithms, on different classification tasks and datasets.
					</aside>
				</section>
				<section>
					<aside class="notes">
						Some of the technology choices here are interesting! This project was done some years ago, so it's fun to look at it in retrospective and reflect on the choices we made. We used GPFlowOpt (now defunct) with HVPoI acquisition function. The models were implemented using Apache MXNet and Gluon API. Plus we implemented DP versions of SGD and Adam by hand. I think it's fair to say that none of that has stood the test of time, and we will see a proof of that in the very next case study.
					</aside>
				</section>
				<section>
					Case study 2: PFairDP - utility vs privacy vs fairness
					<aside class="notes">
						3d fronts, exciting! This second study is a natural extension of the first, were we added one more objective to the optimisation problem: algorithmic fairness. You might be aware of the fact that there are over 20 definitions of fairness, and we managed to build a pipeline that would work with any of these that can be tuned with a hyperparameter.
					</aside>
				</section>
				<section>

					<aside class="notes">
						Specifically, we tried statistical parity difference (SPD) and disparate impact (DI)
					</aside>
				</section>
				<section>

					<aside class="notes">
						Again, we were able to build Pareto fronts to quantify the trade-off here. Interestingly enough, visualising a 3D pareto front is a challenge. Our observations are that our procedure yielded quite sparse fronts, and 3D surfaces need many data points before they become comprehensible when rendered. However giving users interactive 3D render that they can turn and inspect at various angles works pretty good.
					</aside>
				</section>
				<section>

					<aside class="notes">
						But that's not even what excites me the most about this work. Personally, I am mostly excited about the fact that we managed to reproduce, augment and even correct some of the earlier published results. What's important is that those results were achieved manually with constraint setting, and we managed to identify the same trade-offs in the course of one fully automated procedure.
					</aside>
				</section>
				<section>

					<aside class="notes">
						Evolution of tech stack, as promised. This project uses PyTorch tools, including GPyTorch fom modelling, BoTorch for optimisation, and Opacus for differentially private algorithms. Algorithmic fairness definitions are taken from AIF360. We used EHVI as an acquisition function.
					</aside>
				</section>
				<section>
					Case study 3: ECOPt - utility vs energy efficiency
					<aside class="notes">
						Finally, the most recent case study, just finished last year. This time we are back to 2-way trade-offs, matching utility and energy efficiency of ML models. Specifically, the focus of this work is on ML inference, because we felt training is already covered well, while inference is mostly neglected in the literature.
					</aside>
				</section>
				<section>
					
					<aside class="notes">
						This project is a good illustration of the main challenge of setting up these optimisation pipelines. As a community, we are pretty good at training an ML model and measuring its accuracy. But designing for a accurate measurement of other objectives is trickier, and usually takes the majority of time. For example here we had to sift through a large variety of ways of measuring energy consumption, choose from many tools, work around limitations, etc. This is natural of course, but still something to keep in mind.
					</aside>
				</section>
				<section>
					
					<aside class="notes">
						Anyway, we managed to prove that ECOpt is capable of building reasonable Pareto fronts, as for example here. Each of these black points represents a Pareto-optimal trade-off in performance and energy efficiency.
					</aside>
				</section>
				<section>
					
					<aside class="notes">
						Along the way we have shown that number of parameters isn't necessarily a reliable proxy for energy consumption, and got to play with LLMs a bit on different hardwares, to prove our method translates well between platforms.
					</aside>
				</section>
				<section>
					Guide
					<aside class="notes">
						Now, moving on to the final part of the talk. I promised to give you all an advice on how to approach similar trade-offs quantification questions in your domain. So, here are my tips
					</aside>
				</section>
				<section>
					<aside class="notes">
						First, understand what do you really care about. Think about your current project - what trade-off keeps you up at night?
						
						Second, ensure you have a good scalar-valued metric for each of the objectives you are interested in. Remember to normalise them to keep the scales comparable, otherwise one of your metrics will have more weight than the other.
					</aside>
				</section>
				<section>
					<aside class="notes">
						Also on that note, make sure you have enough hyperparameters to actually tune both objectives. It seems obvious, but might be easily overlooked.
					</aside>
				</section>
				<section>
					<aside class="notes">
						Next, implementation of MOBO pipeline. I am not associated with PyTorch ecosystem in any way, but I recommend PyTorch and BoTorch for implementing your experiments. Even though API can be quite awkward at times, I simply haven't found a better library for MOBO.
					</aside>
				</section>
				<section>
					<aside class="notes">
						Third, use EHVI as an acquisition function, unless you have very specific requirements, such as very noise objectives. Recently it was shown that LogEHVI can be used as a stand-in replacement, so perhaps consider using that too.
					</aside>
				</section>
				<section>
					<aside class="notes">
						Of course, prototypes are everything. Run something simple first, where you can find the true Pareto front by just running a very fine grid on the entire HP space. Only after that move on to true experiments.
					</aside>
				</section>
				<section>
					<aside class="notes">
						Run the optimisation, analyse the discovered frontier.
					</aside>
				</section>
				<section>
					<aside class="notes">
						Finally, consider how you might use the discovered front. Is it for compliance, to inform decision making, or simply to confirm your existing thresholds cannot be improved upon?
					</aside>
				</section>
				<section>
					<aside class="notes">
						Thanks, questions
					</aside>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
