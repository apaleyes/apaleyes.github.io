<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Automated discovery of trade-off between accuracy, privacy and fairness in machine learning models
		</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<style>
			.full {
				height: 100%;
				width: 100%;
			}
			.credit {
				bottom: 10px;
				left: 0px;
				position: absolute;
				font-size: medium;
				text-align: left;
			}
			.highlight-red {
				color: #ff2c2d;
			}
			.container{
				display: flex;
			}
			.col{
				flex: 1;
			}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
<section>
	<h3>Automated discovery of trade-off between utility, privacy and fairness in machine learning models</h3>
	<p>
		<small>Bogdan Ficiu, Neil D. Lawrence, Andrei Paleyes</small><br/>
		<small>Department of Computer Science and Technology, University of Cambridge</small>
	</p>
	<p>
		BIAS workshop, ECML 2023
	</p>
	<aside class="notes">
		Hi everyone! My name is Andrei, very happy to be a part of this workshop. I will be presenting the work done with my colleagues Bogdan and Neil.
	</aside>
</section>
<section>
	<div style="height: 99vh">
		<p>
			<img src="images/fairness_vs_priv_vs_accuracy.png" />
		</p>
		<p class="credit">Image source: Zhang et al. 2021 "Balancing Learning Model Privacy, Fairness, and Accuracy With Early Stopping Criteria"</p>
	</div>
	<aside class="notes">
		Our work deals with the question of "How to find the perfect balance between utility, fairness and privacy of an ML model?" If you think about it, you realise that there is a hidden assumption here that these metrics affect each other.
	</aside>
</section>
<section>
	<div style="height: 99vh">
		<p>
			<img src="images/tradeoff_doubt.png" />
		</p>
		<p>But... (Local) Differential Privacy has NO Disparate Impact on Fairness!</p>
	</div>
	<aside class="notes">
		Now if you did not have your cup of coffee this morning (why would you miss on Italian coffee, it's awesome) and still haven't woke up for the previous talk, the speaker right before me just partially questioned of this very assumption. Seriously, please check out their paper, it's good and it shows how to doubt conventional wisdom. But it seems to be no doubt so far that the three-way trade-off is a thing, so the overall premise of our work holds, and I can continue this presentation without being laughed out. 
	</aside>
</section>
<section>
	<div style="height: 99vh">
		<div class="container">
			<div class="col"><img src="images/privacy_constraints.png" style="height: 60vh;" /></div>
			<div class="col"><img src="images/fairness_constraints.png" style="height: 90vh" /></div>
		</div>
		<div class="credit">Table sources: Xu et al. 2019 "Achieving Differential Privacy and Fairness in Logistic Regression", Chang and Shokri 2020 "On the Privacy Risks of Algorithmic Fairness"</div>
	</div>
	<aside class="notes">
		So how do people strike this balance? One common approache we have seen so far is based on constraints. Basically the data scientist sets the level of fairness and privacy they'd like, and trains the ML model under these fixed constraints. Another simple approach is to just try a few hyperparameter settings and choose the one that yields the best results. These ideas are not bad, at the very least because they are relatively simple. But there are two problems with them. For one, they can be very redundant and you may end up evaluating many unnecessary configurations. For two, you might miss good configurations. E.g. maybe you could have achieved the same accuracy of your model with a lower bound on privacy, but with a predefined constraint you will never be able to find this out.
	</aside>
</section>
<section>
	<div style="height: 99vh">
		Find a set of Pareto optimal points <em>x</em> for:
		<img src="images/optimisation.png" style="background-color: white; padding: 5px;" />
	</div>
	<aside class="notes">
		Instead we propose an automated discovery approach. Specifically, we formulate this task as an optimisation problem. We have three objectives: utility, fairness and privacy. This means we have a 3D space of objectives, and we need to find a Pareto frontier in this space - a set of points that are Pareto optimal, meaning improving one objective at this point necessarily deteriorates at least one other.
	</aside>
</section>
<section>
	<span>f(x): PFairDP</span>
	<div style="height: 89vh">
		<img src="images/pipeline.png" />
	</div>
	<aside class="notes">
		We have designed a modular pipeline, which we called PFairDP, to which one can plug in any definition of fairness or utility. The only requirement we have that whatever metric one uses, it needs to have at least one hyperparameter directly affecting this technique. For example for differential privacy we use clipping norm and noise multiplier for DP.
	</aside>
</section>
<section>
	<span>Discovery: Bayesian optimisation</span>
	<div style="height: 89vh">
		<img src="images/bayesopt_ehvi.png" />
		<p class="credit">Image source: Gaudrie 2019 "High-Dimensional Bayesian Multi-Objective Optimization"</p>
	</div>
	<aside class="notes">
		Now, as you might be aware, ML models can be quite time consuming at training. Therefore we need a technique that can efficiently deal with optimising expensive functions. We propose to use multi-objective Bayesian optimisation based on Hypervolume improvement of the Pareto front. This approach is very generic as it can work with many definitions of fairness, privacy and utility.
	</aside>
</section>
<section>
	<h3>Contributions</h3>
	<ul>
		<li>Description of full procedure</li>
		<li>Better than grid search a.k.a. constraint setting</li>
		<li>Replication of existing approaches</li>
	</ul>
	<aside class="notes">
		To summarize our contributions. First, we describe and implemented the full procedure, including the complete training pipeline and the optimisation loop around it. Second, we demonstrate that it is to discover better fronts than grid, which is essentially a constraint setting. Third, we show how to use this procedure to automatically discover results previously described by other authors.
	</aside>
</section>
<section>
	But wait! What about...<br/>
	...visualisation of the fronts?<br/>
	...choice of metrics of utility, fairness and privacy?<br/>
	...computational efficiency?<br/>
	...!?!?<br/>
	<br/><br/>
	<div class="fragment">
		<p>Let's chat at the poster!</p>
		<p>&#9993; <a href="mailto:ap2169@cam.ac.uk">ap2169@cl.cam.ac.uk</a></p>
		<p>Thanks!</p>
	</div>
	<aside class="notes">
		We have basically skipped all the technical details of our work, instead focusing on the intuition behind our work. There are interesting questions to discuss such as computational efficiency, visualisaiton of the fronts, choice of metrics of utility, fairness and privacy, and so on. Please come to our poster later today, or otherwise use the contact information on this slide to ask us questions. Thank you for your attention, and have a good day!
	</aside>
</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				width: "100%",
				height:"100%",
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
